{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On the Notion of the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient descent: Find the minimum of a cost function.\n",
    "\n",
    "<table style = \"width:100%\">\n",
    "    <tr>\n",
    "        <th style = \"width:10%\">Regression</th>\n",
    "        <th style = \"width:35%\">Cost Function Equation</th>\n",
    "        <th>Gradient of Cost Function</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>simple - no intercept</td>\n",
    "        <td>$\\mathcal L = \\sum (y - \\beta x)^2$</td>\n",
    "        <td>$\\frac{d\\mathcal L}{d\\beta} = -2\\sum x(y - \\beta x)$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>simple - with intercept</td>\n",
    "        <td>$\\mathcal L = \\sum (y - (\\beta_1 x + \\beta_0))^2$</td>\n",
    "        <td>$\\nabla\\mathcal L = -2\\sum(y - (\\beta_1 x + \\beta_0))\\hat{\\beta_0}-2\\sum x(y - (\\beta_1 x + \\beta_0))\\hat{\\beta_1}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>multiple</td>\n",
    "        <td>$\\mathcal L = \\sum (y - (\\beta_n x_n + ... + \\beta_1 x_1 + \\beta_0))^2$</td>\n",
    "        <td>$\\nabla\\mathcal L = -2\\sum (y - (\\beta_n x_n + ... + \\beta_1 x_1 + \\beta_0))\\hat{\\beta_0}- ... -2\\sum x_n(y - (\\beta_n x_n + ... + \\beta_1 x_1 + \\beta_0))\\hat{\\beta_n}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>multiple - with ridge regularization</td>\n",
    "        <td>$\\mathcal L = \\sum (y - (\\beta_2 x_2 + \\beta_1 x_1))^2 + \\lambda(\\beta_1^2 + \\beta_2^2)$</td>\n",
    "        <td>$\\nabla\\mathcal L = -2[\\sum (y - (\\beta_2 x_2 + \\beta_1 x_1)) + \\lambda\\beta_1]\\hat{\\beta_1} - 2[\\sum (y - (\\beta_2 x_2 + \\beta_1 x_1)) + \\lambda\\beta_2]\\hat{\\beta_2}$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where Gradient Equals $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LR - no intercept: If $\\frac{d\\mathcal L}{d\\beta} = -2\\sum x(y - \\beta x) = 0$, then $\\sum xy = \\beta\\sum x^2$, so $\\beta = \\frac{\\sum xy}{\\sum x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_features=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.74110031])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression().fit(X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.7411003148779"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X*y) / np.sum(X**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Points in Direction of Steepest Increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![plot](images/contourPlotSSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Code\n",
    "\n",
    "$\\nabla\\mathcal L = -2\\sum(y - (\\beta_1 x + \\beta_0))\\hat{\\beta_0}-2\\sum x(y - (\\beta_1 x + \\beta_0))\\hat{\\beta_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_features=1, bias=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.74110031] 2.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression().fit(X, y)\n",
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(beta_0, beta_1, x=X, y=y):\n",
    "    beta_0_component = -2 * np.sum(y - (beta_1 * x + beta_0))\n",
    "    beta_1_component = -2 * np.sum(x * (y - (beta_1 * x + beta_0)))\n",
    "    return beta_0_component, beta_1_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-8.304468224196171e-14, 1.240140003855395e-12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Close to optimal for $\\beta_0$, off the mark for $\\beta_1$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-179.2246287496094, 3000.3712222450754)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(3, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Close to optimal for $\\beta_1$, off the mark for $\\beta_0$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1594.6228338691285, -123.31656003412012)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(10, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
