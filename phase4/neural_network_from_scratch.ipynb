{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NN by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using NumPy 1.18 and Sklearn 0.24\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to make a very simple neural network by hand.\n",
    "\n",
    "Let's imagine that we have just a five-neuron network: two input neurons, two hidden-layer neurons, and a single output neuron.\n",
    "\n",
    "We'll start by giving ourselves a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_regression(n_features=2,\n",
    "                       n_informative=1,\n",
    "                       noise=100,\n",
    "                       random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's first check to see how a vanilla linear regression model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4144461358278726"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression().fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have just two features, so we'll have two input neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Initial inputs\n",
    "input1 = X[:, 0].reshape(-1, 1)\n",
    "input2 = X[:, 1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weights for the Connections Between the Input Neurons and the Hidden Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll just start by setting our weights randomly. The idea will be that we'll be able to use gradient descent to improve their values during network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Four weights to optimize for between input\n",
    "# and hidden layers\n",
    "\n",
    "# For simplicity let's assume biases of 0\n",
    "# throughout\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "in_hid_weights1 = np.random.rand(2)\n",
    "in_hid_weights2 = np.random.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37454012, 0.95071431])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_hid_weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73199394, 0.59865848])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_hid_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each neuron in the hidden layer will take the two inputs and multiply them by a unique set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "in1_to_hid = (np.sum(np.product([in_hid_weights1, X], axis=0),\n",
    "                     axis=1)).reshape(-1, 1)\n",
    "in2_to_hid = (np.sum(np.product([in_hid_weights2, X], axis=0),\n",
    "                     axis=1)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For simplicity let's assume a **linear activation function**, $f(x)=x$, in the hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weights for the Connections Between the Hidden Neurons and the Output Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again we'll just set our weights randomly. Here there will be two weights: one governing the connection between each hidden neuron and the output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "\n",
    "hid_out_weights = np.random.rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11505457, 0.60906654])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hid_out_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we need to take the contribution from each hidden neuron and create a linear sum with the predetermined weights, just as above in the hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "joint_to_out = np.hstack((in1_to_hid, in2_to_hid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output = (np.sum(np.product([hid_out_weights, joint_to_out], axis=0),\n",
    "                 axis=1)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again we'll assume a linear activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we can compute a measure of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.15409393186755"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.flatten()\n",
    "\n",
    "np.sqrt(mse(y, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now then: How do we make corrections to our weights to improve our model's performance? Our network looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![nn](images/SimpleNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly our output is a function of these six weights. But what function, exactly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In the top hidden node we construct: <br/> $H_1 = w_1X_1 + w_3X_2$ <br/> and in the bottom node we construct: <br/> $H_2 = w_2X_1 + w_4X_2$.\n",
    "<br/> <br/>\n",
    "- In the output node we construct: <br/> $\\hat{y} = w_5H_1 + w_6H_2$ <br/> i.e. <br/> $\\hat{y} = X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our loss function (let's assume) is just $\\mathcal{L} = \\Sigma\\left(y - \\hat{y}\\right)^2$. What are the partial derivatives of this function?\n",
    "\n",
    "We have $\\mathcal{L} = \\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)^2$.\n",
    "\n",
    "Therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our loss function (let's assume) is just $\\mathcal{L} = \\Sigma\\left(y - \\hat{y}\\right)^2$. What are the partial derivatives of this function?\n",
    "\n",
    "We have $\\mathcal{L} = \\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)^2$.\n",
    "\n",
    "Therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- $\\tiny\\frac{\\partial\\mathcal{L}}{\\partial w_1} = -2w_5X_1\\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)$\n",
    "\n",
    "- $\\tiny\\frac{\\partial\\mathcal{L}}{\\partial w_2} = -2w_6X_1\\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)$\n",
    "\n",
    "- $\\tiny\\frac{\\partial\\mathcal{L}}{\\partial w_3} = -2w_5X_2\\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)$\n",
    "\n",
    "- $\\tiny\\frac{\\partial\\mathcal{L}}{\\partial w_4} = -2w_6X_2\\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)$\n",
    "\n",
    "- $\\tiny\\frac{\\partial\\mathcal{L}}{\\partial w_5} = -2(w_1X_1 + w_3X_2)\\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)$\n",
    "\n",
    "- $\\tiny\\frac{\\partial\\mathcal{L}}{\\partial w_6} = -2(w_2X_1 + w_4X_2)\\Sigma\\left(y - [X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)]\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So the goal now should just be to nudge each of our weights in (the opposites of) these directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's first isolate each of these six weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1 = in_hid_weights1[0]\n",
    "w2 = in_hid_weights1[1]\n",
    "w3 = in_hid_weights2[0]\n",
    "w4 = in_hid_weights2[1]\n",
    "w5 = hid_out_weights[0]\n",
    "w6 = hid_out_weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's build an SGD function that will adjust weights after each training sample runs through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def stoch_grad_desc(pred1=input1, pred2=input2, y=y, w1=w1, w2=w2, w3=w3,\n",
    "                    w4=w4, w5=w5, w6=w6, times_thru=1, lr=1e-4):\n",
    "    for k in range(times_thru):\n",
    "        for j in range(pred1.shape[0]):\n",
    "            in1_val = pred1[j]\n",
    "            in2_val = pred2[j]\n",
    "            sum_ = y[j] - in1_val*(w1*w5 + w2*w6)\\\n",
    "                - in2_val*(w3*w5 + w4*w6)\n",
    "            w5 += lr*(w1*in1_val + w3*in2_val)*sum_\n",
    "            w6 += lr*(w2*in1_val + w4*in2_val)*sum_\n",
    "            w1 += lr*w5*in1_val*sum_\n",
    "            w2 += lr*w6*in1_val*sum_\n",
    "            w3 += lr*w5*in2_val*sum_\n",
    "            w4 += lr*w6*in2_val*sum_\n",
    "            output = pred1*(w1*w5 + w2*w6) + pred2*(w3*w5 + w4*w6)\n",
    "            if j == 0 and k == 0:\n",
    "                print(f\"\"\"After a single data point our RMSE is {np.sqrt(mse(y, output.flatten()))}\"\"\")\n",
    "        print(f\"\"\"After {k+1} epochs our RMSE is {np.sqrt(mse(y, output.flatten()))}\"\"\")\n",
    "    return w1, w2, w3, w4, w5, w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a single data point our RMSE is 132.0718494436695\n",
      "After 1 epochs our RMSE is 130.71260165965495\n",
      "After 2 epochs our RMSE is 125.90039567221291\n",
      "After 3 epochs our RMSE is 114.90068010279187\n",
      "After 4 epochs our RMSE is 105.45570922177163\n",
      "After 5 epochs our RMSE is 102.94826034137003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.22727686]),\n",
       " array([8.73560832]),\n",
       " array([0.87579055]),\n",
       " array([0.97998175]),\n",
       " array([3.12714953]),\n",
       " array([8.44527613]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoch_grad_desc(times_thru=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Easy Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our error has decreased with more epochs, but to illustrate the full power of our network let's see if it can find the right coefficients for an \"easy\" problem where there is a strong correlation between both of two input features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the problem\n",
    "\n",
    "X_easy, y_easy = make_regression(n_features=2, n_informative=2,\n",
    "                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Again defining weights randomly. We'll be using the same network,\n",
    "# so we need six weights.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "w_easy = np.random.rand(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After a single data point our RMSE is 105.2827433708648\n",
      "After 1 epochs our RMSE is 0.0003058349135338329\n",
      "After 2 epochs our RMSE is 5.040642635748519e-09\n",
      "After 3 epochs our RMSE is 1.2706999512590757e-13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.556340807199361,\n",
       " 10.75555705679945,\n",
       " 6.914105827696129,\n",
       " 7.997420739574159,\n",
       " 4.334717820507707,\n",
       " 5.515048597227902)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying our stoch_grad_desc() function\n",
    "\n",
    "final_weights = stoch_grad_desc(pred1=X_easy[:, 0],\n",
    "                                pred2=X_easy[:, 1],\n",
    "                                y=y_easy,\n",
    "         w1=w_easy[0], w2=w_easy[1], w3=w_easy[2],\n",
    "         w4=w_easy[3], w5=w_easy[4], w6=w_easy[5],\n",
    "         lr=4e-3, times_thru=3)\n",
    "final_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing with `LinearRegression()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can translate these final weight values into $\\beta_1$ and $\\beta_2$ for a traditional linear regression $\\hat{y} = \\beta_1X_1 + \\beta_2X_2$.\n",
    "\n",
    "Above we calculated $\\hat{y} = X_1(w_1w_5 + w_2w_6) + X_2(w_3w_5 + w_4w_6)$.\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "- $\\beta_1 = w_1w_5 + w_2w_6$, and\n",
    "- $\\beta_2 = w_3w_5 + w_4w_6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plugging these in for our final calculated weights we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our mini-NN found coefficients of 87.73730719 and 74.07686178.\n"
     ]
    }
   ],
   "source": [
    "beta1 = final_weights[0]*final_weights[4] +\\\n",
    "    final_weights[1]*final_weights[5]\n",
    "beta2 = final_weights[2]*final_weights[4] +\\\n",
    "    final_weights[3]*final_weights[5]\n",
    "print(f\"Our mini-NN found coefficients of {np.round(beta1, 8)}\\\n",
    " and {np.round(beta2, 8)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare these numbers with the results of `LinearRegression()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([87.73730719, 74.07686178])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression(fit_intercept=False).fit(X_easy, y_easy).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our little neural network, which has:\n",
    "- only five neurons;\n",
    "- randomly generated initial weights;\n",
    "- only linear activation functions; and\n",
    "- all biases set to 0\n",
    "\n",
    "can do the same job as a linear regression after just a couple of epochs!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
